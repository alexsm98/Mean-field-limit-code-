{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mechanical_regression import MechanicalRegression\n",
    "from vizual import loss_plot, alpha1_plot \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from cycler import cycler\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from multiprocessing import Pool\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiments(pb_type, L, activ, train_data_dict, test_data, p, D2, nsteps=100, lr=0.01):\n",
    "    \"\"\"Results for fixed model architecture and different number of training points\n",
    "    Parameters\n",
    "        - pb_type: str, 'classification' or 'regression' type of problem\n",
    "        - L: int, number of layers\n",
    "        - activ: str, activation function must be one of 'ReLU', 'tanh' or 'sigmoid'\n",
    "        - train_data_dict: dict, datasets of different sizes used to train the model \n",
    "        - test_data: tuple, test dataset\n",
    "        - p: int, dimension of inputs \n",
    "        - D2: int, dimension of random feature map psi2 \n",
    "        - nsteps: int, number of steps for the optimization\n",
    "        - lr: float, learning rate for the optimization \n",
    "    Returns \n",
    "        - alpha1_energy: dict, energy as described in the paper \n",
    "        - train_l2_reg_loss: dict, L2 optimized ridge loss on training sets \n",
    "        - train_error: dict, training error is MSE for regression and 1-accuracy for classification \n",
    "        - test_error: dict, test error is MSE for regression and 1-accuracy for classification \n",
    "    \"\"\"\n",
    "    alpha1_energy, train_l2_reg_loss, train_error, test_error = {}, {}, {}, {}\n",
    "    model = MechanicalRegression(pb_type, L, activ, p, D2)\n",
    "    x_test, y_test = test_data\n",
    "    for N in train_data_dict:\n",
    "        x_train, y_train = train_data_dict[N]\n",
    "        alpha1 = model.fit(x_train, y_train, nsteps=nsteps, lr=lr) \n",
    "        if L > 0:\n",
    "            alpha1_energy[N] = (1/L)*torch.sum(alpha1 * alpha1)\n",
    "        else:\n",
    "            alpha1_energy[N] = torch.sum(alpha1 * alpha1)\n",
    "        train_l2_reg_loss[N] = model.get_loss(x_train, y_train, alpha1)\n",
    "        train_error[N] = model.get_error(x_train, y_train)                      \n",
    "        test_error[N] = model.get_error(x_test, y_test)\n",
    "\n",
    "    return alpha1_energy, train_l2_reg_loss, train_error, test_error\n",
    "\n",
    "\n",
    "def experiments_dict(pb_type, activations, Layers, train_data_dict, test_data, p, D2, nsteps, lr):\n",
    "    \"\"\"Experiments for a dataset with different activationd and number of Layers\n",
    "    Parameters:\n",
    "        - pb_type, train_data_dict, test_data, p, D2, nsteps, lr: same as in function 'experiments'\n",
    "        - activations: list, different activations to use\n",
    "        - Layers, list, different number of layers to use\n",
    "    Returns:\n",
    "        - results: dict, metrics for trained models with different activations and layers \n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for activ in activations:\n",
    "        results[activ] = {}\n",
    "        for L in Layers:\n",
    "            res = experiments(pb_type, L, activ, train_data_dict, test_data, p, D2, nsteps, lr)\n",
    "            results[activ][L] = {name: metric for name, metric in zip(metric_names, res)}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of metrics and activations to use \n",
    "\n",
    "activations = ['ReLU', 'tanh', 'sigmoid']\n",
    "metric_names = ['alpha1_energy', 'l2_reg_loss', 'train_error', 'test_error']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression data\n",
    "\n",
    "## Synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_train(n, sigma=1):\n",
    "    \"\"\"Train data for regression with synthetic dataset\"\"\"\n",
    "    np.random.seed(111)\n",
    "    x = (np.linspace(-5, 5, n))[..., np.newaxis]\n",
    "    noise = np.random.normal(scale=sigma, size=(n, 1))\n",
    "    y = x**2 + x + noise\n",
    "    return torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "def get_data_test(n, sigma=1):\n",
    "    \"\"\"Test data for regression with synthetic dataset\"\"\"\n",
    "    np.random.seed(111)\n",
    "    x = (np.linspace(-5, 5, n) + 0.5/n)[..., np.newaxis]\n",
    "    y = x**2 + x + np.random.normal(scale=sigma, size=(n, 1))\n",
    "    return torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "def get_data_test_new(n, sigma=1):\n",
    "    \"\"\"New test data for regression with synthetic dataset\"\"\"\n",
    "    np.random.seed(111)\n",
    "    x1 = (np.linspace(-7.5, -5, n//2))[..., np.newaxis]\n",
    "    x2 = (np.linspace(5, 7.5, n//2))[..., np.newaxis]\n",
    "    x = np.concatenate((x1,x2), axis=0)\n",
    "    y =x**2 + x + np.random.normal(scale=sigma, size=(n, 1))\n",
    "    return torch.from_numpy(x), torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = get_data_test_new(200, 1)\n",
    "\n",
    "N_list = [50, 100, 200, 400, 800, 1600] \n",
    "data_dict = {N: get_data_train(N, 1) for N in N_list}\n",
    "\n",
    "Layers = [0, 2, 4, 6, 8]\n",
    "pb_type = 'regression'\n",
    "\n",
    "p = test_data[0].shape[1]\n",
    "D2 = 10*p\n",
    "      \n",
    "results_synt = experiments_dict(pb_type, activations, Layers, data_dict, test_data, p, D2, nsteps=500, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha1_plot(results_synt, log_scale_x=False)#, save_path='alpha1_energy_synthetic.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for activ in activations:\n",
    "    loss_plot(results_synt[activ], sharey=False, log_scale_x=False)#, save_path=f'loss_error_synthetic_{activ}.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston Housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_split(X, y, test_size):\n",
    "    \"\"\"Dataset split into train/test\"\"\"\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    y_train, y_test = y_train[..., np.newaxis], y_test[..., np.newaxis]\n",
    "    return torch.tensor(x_train), torch.tensor(x_test), torch.tensor(y_train), torch.tensor(y_test)\n",
    "\n",
    "def get_data_dict(X, y, R=np.arange(0.1, 1.1, 0.1), split=True, train_size = 0.9):\n",
    "    \"\"\"Data dictionary for experiments\"\"\"\n",
    "    if split:\n",
    "        x_train, x_test, y_train, y_test = get_data_split(X, y, test_size=1-train_size)\n",
    "    else:\n",
    "        x_train, y_train = X, y\n",
    "    N_train = len(x_train)\n",
    "    data_train_dict = {}\n",
    "    for r in R:\n",
    "        N = np.int(r * N_train)\n",
    "        x, y = x_train[0:N], y_train[0:N]\n",
    "        data_train_dict[N] = (x, y)\n",
    "    if split:    \n",
    "        return data_train_dict, (x_test, y_test)\n",
    "    else:\n",
    "        return data_train_dict\n",
    "    \n",
    "Xboston, yboston = load_boston(return_X_y = True)\n",
    "boston_train_data_dict, boston_test_data = get_data_dict(Xboston, yboston)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Xboston.shape[1]\n",
    "D2 = 130\n",
    "\n",
    "boston_results = experiments_dict(pb_type, activations, Layers, boston_train_data_dict, \n",
    "                                  boston_test_data, p, D2, nsteps=500, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha1_plot(boston_results, log_scale_x=False)#, save_path='alpha1_energy_boston.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for activ in activations:\n",
    "    loss_plot(boston_results[activ], sharey=False, log_scale_x=False)#, save_path=f'loss_error_boston_{activ}.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(loadmnist):\n",
    "    Ntrain = 50000\n",
    "    Ntest = 10000\n",
    "    if loadmnist:\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    else:\n",
    "        mnist = tf.keras.datasets.fashion_mnist\n",
    "        class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                       'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "        \n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train.astype('float64'), x_test.astype('float64')\n",
    "    x_train, y_train = x_train[0:Ntrain,:,:], y_train[0:Ntrain]\n",
    "    x_test, y_test = x_test[0:Ntest,:,:], y_test[0:Ntest]\n",
    "    \n",
    "    # get one-hot vectors \n",
    "    Y_train = np.zeros((Ntrain,10))\n",
    "    Y_test = np.zeros((Ntest,10))\n",
    "    for i in range(10):\n",
    "        Y_train[y_train[0:Ntrain] == i,i] = 1\n",
    "        Y_test[y_test[0:Ntest] == i,i] = 1\n",
    "    \n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    normalizedata = False\n",
    "    if normalizedata:\n",
    "        a = np.sqrt(np.sum(x_train*x_train,(1,2)))\n",
    "        a = a[:,None,None]+np.zeros((Ntrain,28,28))\n",
    "        x_train = np.divide(x_train , a)\n",
    "        a = np.sqrt(np.sum(x_test*x_test,(1,2)))\n",
    "        a = a[:,None,None]+np.zeros((Ntest,28,28))\n",
    "        x_test = np.divide(x_test , a)\n",
    "        a = 0  \n",
    "    d2 = 28*28\n",
    "    x_train, x_test = np.reshape(x_train, (Ntrain, d2)), np.reshape(x_test, (Ntest, d2))\n",
    "    \n",
    "    x_train = torch.tensor(x_train.astype('float64'))\n",
    "    x_test = torch.tensor(x_test.astype('float64'))\n",
    "    Y_train = torch.tensor(Y_train.astype('float64'))\n",
    "    Y_test = torch.tensor(Y_test.astype('float64'))\n",
    "    \n",
    "    return (x_train, Y_train), (x_test, Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, y), mnist_test_data = load_data(loadmnist=True)\n",
    "mnist_data_dict = get_data_dict(X, y, R=np.arange(0.01, 0.11, 0.01), split=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_type = 'classification'\n",
    "Layers = [0, 1, 2, 3]\n",
    "d = x_test.shape[1]\n",
    "mnist_results = experiments_dict(pb_type, activations, Layers, mnist_data_dict, mnist_test_data, \n",
    "                                      nsteps=100, lr=0.001, p=d, D2=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha1_plot(mnist_results, sharey=False, log_scale_x=False)#, save_path='alpha1_energy_mnist.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for activ in activations:\n",
    "    loss_plot(mnist_results[activ], sharey=False, log_scale_x=False)#, save_path=f'loss_error_mnist_{activ}.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X, y), mnist_fash_test_data = load_data(loadmnist=False)\n",
    "mnist_fash_data_dict = get_data_dict(X, y, R=np.arange(0.01, 0.11, 0.01), split=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train, y_train = mnist_fash_data_dict[list(mnist_fash_data_dict.keys())[-1]]\n",
    "#d = x_test.shape[1]\n",
    "#model = MechanicalRegression(pb_type, 1, 'relu', p=d, D2=d)\n",
    "#alpha1 = model.fit(x_train, y_train, nsteps=60, lr=0.001)\n",
    "#plt.plot(model.loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_type = 'classification'\n",
    "Layers = [0, 1, 2, 3]\n",
    "d = x_test.shape[1]\n",
    "mnist_fash_results = experiments_dict(pb_type, activations, Layers, mnist_fash_data_dict, mnist_fash_test_data, \n",
    "                                      nsteps=100, lr=0.001, p=d, D2=d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha1_plot(mnist_results, sharey=False, log_scale_x=False)#, save_path='alpha1_energy_mnist_fash.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for activ in activations:\n",
    "    loss_plot(mnist_results[activ], sharey=False, log_scale_x=False)#, save_path=f'loss_error_mnist_fash_{activ}.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
