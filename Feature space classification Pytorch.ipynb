{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "#from IPython import get_ipython\n",
    "#get_ipython().magic('reset -sf')\n",
    "\n",
    "import torch\n",
    "torch.cuda.is_available()\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import autograd.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sys import stdout\n",
    "from autograd import grad\n",
    "import time \n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "\n",
    "devicecpu=torch.device(\"cpu\")\n",
    "\n",
    "#plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return result of an input after propagation in space F_2\n",
    "\n",
    "def xtopsi2_torch(x):\n",
    "    if random2 == \"true\":\n",
    "        p=x.shape[1]  #train input dimension --> column vector with 100 rows\n",
    "        np.random.seed(11)\n",
    "        bias2=torch.from_numpy(0.1*np.random.randn(D2))  #b_2^2 with distribution 0.1 * N(0,1) \n",
    "        W2=torch.from_numpy(1.5*np.divide(np.random.randn(p, D2),np.sqrt(p)))   \n",
    "        #W_2^2 with corresponding distribution\n",
    "        e=torch.matmul(x,W2)+bias2\n",
    "    else: \n",
    "        e=x\n",
    "        \n",
    "    if activ == \"relu\":\n",
    "        e=torch.clamp(e, min=0)  #activation function ReLU a(.) = max(0,.)\n",
    "    if activ == \"sigmoid\":\n",
    "        e =  1/(1 + torch.exp(-e))\n",
    "    if activ == \"tanh\":\n",
    "        e = torch.tanh(e)\n",
    "    \n",
    "    if random2 == \"false\":\n",
    "        e = torch.cat([e,torch.ones((x.shape[0],1))],dim=1)\n",
    "    #e=np.cos(e)\n",
    "    return torch.transpose(e, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getalpha2_torch(vx_train,y_train):   \n",
    "    psi2_train=xtopsi2_torch(vx_train)  \n",
    "    A=torch.matmul(psi2_train,torch.transpose(psi2_train,0,1)) + len(y_train)*gamma2*torch.eye(D2)\n",
    "    b=torch.matmul(psi2_train,y_train)\n",
    "    #alpha2=torch.linalg.solve(A,b)\n",
    "    alpha2=torch.linalg.solve(A,b)\n",
    "    loss2=(1/len(y_train))*torch.sum((torch.matmul(torch.transpose(psi2_train,0,1),alpha2) - y_train)**2)\n",
    "    loss2=loss2+gamma2*torch.sum(alpha2**2)\n",
    "    return (loss2,alpha2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain the fitted values \n",
    "def getYpred(x,alpha2):\n",
    "    psi2=xtopsi2_torch(x)\n",
    "    pred=torch.matmul(torch.transpose(psi2,0,1), alpha2)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xtopsi1_torch(x):\n",
    "    if random1 == \"true\":\n",
    "        np.random.seed(100)\n",
    "        p=x.shape[1]  #train input dimension --> column vector with 100 rows\n",
    "        bias1=torch.from_numpy(0.1*np.random.randn(D1))   #b_1 with distribution 0.1 * N(0,1) \n",
    "        W1=torch.from_numpy(1.5*np.divide(np.random.randn(p, D1),np.sqrt(p)))  #W_1 with corresponding distribution \n",
    "        e=torch.matmul(x,W1)+bias1\n",
    "    else: \n",
    "        e=x\n",
    "        \n",
    "    if activ == \"relu\":\n",
    "        e=torch.clamp(e, min=0)  #activation function ReLU a(.) = max(0,.)\n",
    "    if activ == \"sigmoid\":\n",
    "        e =  1./(1 + torch.exp(-e))\n",
    "    if activ == \"tanh\":\n",
    "        e = torch.tanh(e)\n",
    "    \n",
    "    if random1 == \"false\":\n",
    "        e = torch.cat([e,torch.ones((x.shape[0],1))],dim=1)\n",
    "    #e=np.cos(e)\n",
    "    return torch.transpose(e, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_torch(x,alpha1):\n",
    "    for i in range(L):\n",
    "        xtopsi1_transp=torch.transpose(xtopsi1_torch(x),0,1)\n",
    "        x=x+torch.matmul(xtopsi1_transp,alpha1[:,:,i])  #h is removed \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "def loss_torch(alpha1):\n",
    "    loss1=0.5*L*torch.sum(alpha1*alpha1)\n",
    "    phivx_train=phi_torch(vx_train,alpha1)\n",
    "#    phivx_train=vx_train\n",
    "    (loss2,alpha2)=getalpha2_torch(phivx_train,y_train)\n",
    "    return nu*loss1+loss2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Error_plot(init_train_loss,final_train_loss,init_test_loss,final_test_loss,inloss,finloss,dataset):\n",
    "    #perc_list = np.arange(10,90,10)\n",
    "    #N_list = np.array([50,100,200,400,800,1600])\n",
    "    #N_list = np.arange(10,110,10)\n",
    "    Ntrain=5000\n",
    "    N_list=[np.int(rate*Ntrain) for rate in np.arange(0.1,1.1,0.1)]\n",
    "    fig = plt.figure(figsize=(16,4)) \n",
    "    c=1\n",
    "\n",
    "    plt.subplot(1, 3, c)\n",
    "    plt.title('Training Error')\n",
    "    plt.plot(N_list,init_train_loss,label='Ridge regression')\n",
    "    plt.plot(N_list,final_train_loss, label='Mechanical regression')\n",
    "    #plt.plot(perc_list,ridge_loss_train,label='ridge')\n",
    "    plt.xlabel(r'$N_{train}$')\n",
    "    plt.ylabel('Error')\n",
    "    #plt.legend()\n",
    "    c+=1\n",
    "\n",
    "    plt.subplot(1, 3, c)\n",
    "    plt.title('Testing Error')\n",
    "    plt.plot(N_list,init_test_loss,label='Ridge regression')\n",
    "    plt.plot(N_list,final_test_loss,label='Mechanical regression')\n",
    "    #plt.plot(perc_list,ridge_loss_test,label='ridge')\n",
    "    plt.xlabel(r'$N_{train}$')\n",
    "    plt.ylabel('Error')\n",
    "    #plt.legend()\n",
    "   # plt.yscale('log')\n",
    "    c+=1\n",
    "\n",
    "    plt.subplot(1, 3, c)\n",
    "    plt.title(r\"$L_2$ regularized loss\")\n",
    "    plt.plot(N_list,inloss,label='Ridge regression')\n",
    "    plt.plot(N_list,finloss,label='Mechanical regression')\n",
    "    plt.xlabel(r'$N_{train}$')\n",
    "    plt.legend()\n",
    "    #plt.ylabel('MSE')\n",
    "    \n",
    "\n",
    "    #plt.savefig('Error_{}_{}_Adam.pdf'.format(dataset,activ))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_alpha1(alpha1_init,nsteps=1000,lr=0.1):\n",
    "    alpha1 = Variable(alpha1_init ,requires_grad=True)\n",
    "    opt = torch.optim.Adam([alpha1],lr=lr)\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    for i in range(nsteps):\n",
    "        opt.zero_grad()\n",
    "        stdout.write(\"\\r[%s]\" % (i+1)) \n",
    "        loss = loss_torch(alpha1) # Calculate loss function\n",
    "        loss_history.append(loss.detach().numpy()) # Do some recordings for plots\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "       # if i % 50 == 0:\n",
    "       #     print(f'iter {i+1}: loss = {loss}')\n",
    "    \n",
    "    #print(f'final loss: loss = {loss}')\n",
    "\n",
    "    print(\"Min loss after training alpha : {}\".format(np.min(loss_history)))\n",
    "\n",
    "    plt.figure()    \n",
    "    plt.plot(loss_history)                   #plot the loss as a function of iteration i\n",
    "    plt.title('N= {}, lr= {}'.format(len(vx_train),lr))\n",
    "    plt.show()\n",
    "    \n",
    "    return alpha1 , loss_history\n",
    "\n",
    "        #stdout.write(\"\\r[%s]\" % loss) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define prediction for classification\n",
    "\n",
    "def gettesterror_classif(pred,y):\n",
    "    true_labels = np.argmax(y,axis=1)\n",
    "    pred_labels=torch.argmax(pred,axis=1)\n",
    "    correct=(pred_labels==true_labels)\n",
    "    error=1-np.sum(correct.numpy())/len(y)\n",
    "    return error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getYpred(x,alpha2):\n",
    "    psi2=xtopsi2_torch(x)\n",
    "    pred=torch.matmul(torch.transpose(psi2,0,1), alpha2)\n",
    "    return pred\n",
    "\n",
    "def get_error(x,y,alpha2):\n",
    "    pred=getYpred(x,alpha2)\n",
    "    true_labels=np.argmax(y,axis=1)\n",
    "    pred_labels=torch.argmax(pred,axis=1)\n",
    "    correct=(pred_labels==true_labels)\n",
    "    error=1-np.sum(correct.numpy())/len(y)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loaddata(rate):\n",
    "    loadmnist=False\n",
    "    Ntrain=np.int(rate*5000)\n",
    "    Ntest=10000\n",
    "    if loadmnist:\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        class_names = ['0', '1', '2', '3', '4',\n",
    "                   '5', '6', '7', '8', '9']\n",
    "    else:\n",
    "        mnist = tf.keras.datasets.fashion_mnist\n",
    "        class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                   'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train=x_train.astype('float64')\n",
    "    x_test=x_test.astype('float64')\n",
    "    x_train=x_train[0:Ntrain,:,:]\n",
    "    y_train=y_train[0:Ntrain]\n",
    "    x_test=x_test[0:Ntest,:,:]\n",
    "    y_test=y_test[0:Ntest]\n",
    "    Y_train=np.zeros((Ntrain,10))\n",
    "    Y_test=np.zeros((Ntest,10))\n",
    "    for i in range(10):\n",
    "        Y_train[y_train[0:Ntrain]==i,i]=1\n",
    "        Y_test[y_test[0:Ntest]==i,i]=1\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    normalizedata=False\n",
    "    if normalizedata:\n",
    "        a=np.sqrt(np.sum(x_train*x_train,(1,2)))\n",
    "        a=a[:,None,None]+np.zeros((Ntrain,28,28))\n",
    "        x_train = np.divide(x_train , a)\n",
    "        a=np.sqrt(np.sum(x_test*x_test,(1,2)))\n",
    "        a=a[:,None,None]+np.zeros((Ntest,28,28))\n",
    "        x_test = np.divide(x_test , a)\n",
    "        a=0 \n",
    "    show=False\n",
    "    if show == True:\n",
    "        def showimages(x_train,y_train):\n",
    "            plt.figure(figsize=(10,10))\n",
    "            for i in range(25):\n",
    "                plt.subplot(5,5,i+1)\n",
    "                plt.xticks([])\n",
    "                plt.yticks([])\n",
    "                plt.grid(False)\n",
    "                plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "                plt.xlabel(class_names[y_train[i]])\n",
    "            plt.show()\n",
    "        showimages(x_train,y_train)\n",
    "    d2=28*28\n",
    "    vx_train=np.reshape(x_train,(Ntrain,d2))\n",
    "    vx_test=np.reshape(x_test,(Ntest,d2))\n",
    "    return (vx_train,Y_train,vx_test,Y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadmnist=False\n",
    "\n",
    "def get_sets(rate):\n",
    "    (vx_train,y_train,vx_test,y_test)=Loaddata(rate)\n",
    "\n",
    "    vx_train=torch.tensor(vx_train.astype('float64'))\n",
    "    vx_test=torch.tensor(vx_test.astype('float64'))\n",
    "    y_train=torch.tensor(y_train.astype('float64'))\n",
    "    y_test=torch.tensor(y_test.astype('float64'))\n",
    "\n",
    "    return (vx_train, y_train), (vx_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store -r alpha1_mnist1L_tanh\n",
    "#%store -r loss_mnist1L_relu_NEW \n",
    "#%store -r train_mnist1L_relu_NEW \n",
    "#%store -r test_mnist1L_relu_NEW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gamma2=0.01\n",
    "nu=0.01\n",
    "\n",
    "\n",
    "p = 784 # vx_train.shape[1]\n",
    "D1 = p + 1\n",
    "D2 = 784\n",
    "\n",
    "Layers=[1,2,3]\n",
    "\n",
    "\n",
    "random1=\"false\"\n",
    "random2=\"true\"\n",
    "\n",
    "\n",
    "# activ = relu\n",
    "\n",
    "activ=\"relu\"\n",
    "\n",
    "alpha1min_list_fash_relu = []\n",
    "loss_list_fash_relu = []\n",
    "train_list_fash_relu = []\n",
    "test_list_fash_relu = []\n",
    "\n",
    "\n",
    "\n",
    "for l in Layers:\n",
    "    \n",
    "    L=l\n",
    "    alpha1min_list = []\n",
    "    loss_list = []\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    opt_time = []\n",
    "\n",
    "\n",
    "    for rate in np.arange(0.1,1.1,0.1):\n",
    "\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        # get the data\n",
    "        loadmnist=False \n",
    "        (vx_train, y_train), (vx_test, y_test) = get_sets(rate)\n",
    "\n",
    "        Ntrain=vx_train.shape[0]\n",
    "        Ntest=vx_test.shape[0]\n",
    "\n",
    "        # optimize over alpha\n",
    "        lr=0.001\n",
    "        alpha1min, loss_history = optimize_alpha1(torch.zeros((D1,p,L),dtype=torch.float64),nsteps=200,lr=lr)\n",
    "        lmin = loss_torch(alpha1min).detach().numpy()\n",
    "\n",
    "        # append the loss / alpha\n",
    "        loss_in=loss_torch(torch.zeros((D1,p,L),dtype=torch.float64)).numpy()\n",
    "        loss_list.append([loss_in,lmin])\n",
    "        alpha1min_list.append(alpha1min.detach().numpy())\n",
    "\n",
    "\n",
    "        # append the train/test error\n",
    "        _,alpha2min=getalpha2_torch(phi_torch(vx_train,alpha1min),y_train)\n",
    "        _,alpha2_init=getalpha2_torch(vx_train,y_train)\n",
    "\n",
    "        inierror_train=get_error(vx_train,y_train,alpha2_init)\n",
    "        finerror_train=get_error(phi_torch(vx_train,alpha1min),y_train,alpha2min)\n",
    "\n",
    "        inierror_test=get_error(vx_test,y_test,alpha2_init)\n",
    "        finerror_test=get_error(phi_torch(vx_test,alpha1min),y_test,alpha2min)\n",
    "\n",
    "        print('initial {} error= {}'.format(\"train\",np.round(inierror_train,3))) \n",
    "        print('final {} error= {}'.format(\"train\",np.round(finerror_train,3)))\n",
    "\n",
    "        print('initial {} error= {}'.format(\"test\",np.round(inierror_test,3)))\n",
    "        print('final {} error= {}'.format(\"test\",np.round(finerror_test,3)))\n",
    "\n",
    "        # append the train/test error\n",
    "        #inierror_train,finerror_train,inierror_test,finerror_test = get_error(vx_train,y_train,vx_test,y_test)\n",
    "\n",
    "        train_loss.append(np.array([inierror_train,finerror_train]))\n",
    "        test_loss.append((np.array([inierror_test,finerror_test])))\n",
    "\n",
    "        end = time.perf_counter()\n",
    "        print(f\"Optimization time: {end-start}\")\n",
    "\n",
    "\n",
    "    alpha1min_list_fash_relu.append(alpha1min_list)\n",
    "    loss_list_fash_relu.append(loss_list)\n",
    "    train_list_fash_relu.append(train_loss)\n",
    "    test_list_fash_relu.append(test_loss)\n",
    "    \n",
    "\n",
    "    \n",
    "# activ = tanh  \n",
    "\n",
    "activ=\"tanh\"\n",
    "\n",
    "alpha1min_list_fash_tanh = []\n",
    "loss_list_fash_tanh = []\n",
    "train_list_fash_tanh = []\n",
    "test_list_fash_tanh = []\n",
    "\n",
    "\n",
    "\n",
    "for l in Layers:\n",
    "    \n",
    "    L=l\n",
    "    alpha1min_list = []\n",
    "    loss_list = []\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    opt_time = []\n",
    "\n",
    "\n",
    "    for rate in np.arange(0.1,1.1,0.1):\n",
    "\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        # get the data\n",
    "        loadmnist=False \n",
    "        (vx_train, y_train), (vx_test, y_test) = get_sets(rate)\n",
    "\n",
    "        Ntrain=vx_train.shape[0]\n",
    "        Ntest=vx_test.shape[0]\n",
    "\n",
    "        # optimize over alpha\n",
    "        lr=0.001\n",
    "        alpha1min, loss_history = optimize_alpha1(torch.zeros((D1,p,L),dtype=torch.float64),nsteps=200,lr=lr)\n",
    "        lmin = loss_torch(alpha1min).detach().numpy()\n",
    "\n",
    "        # append the loss / alpha\n",
    "        loss_in=loss_torch(torch.zeros((D1,p,L),dtype=torch.float64)).numpy()\n",
    "        loss_list.append([loss_in,lmin])\n",
    "        alpha1min_list.append(alpha1min.detach().numpy())\n",
    "\n",
    "\n",
    "        # append the train/test error\n",
    "        _,alpha2min=getalpha2_torch(phi_torch(vx_train,alpha1min),y_train)\n",
    "        _,alpha2_init=getalpha2_torch(vx_train,y_train)\n",
    "\n",
    "        inierror_train=get_error(vx_train,y_train,alpha2_init)\n",
    "        finerror_train=get_error(phi_torch(vx_train,alpha1min),y_train,alpha2min)\n",
    "\n",
    "        inierror_test=get_error(vx_test,y_test,alpha2_init)\n",
    "        finerror_test=get_error(phi_torch(vx_test,alpha1min),y_test,alpha2min)\n",
    "\n",
    "        print('initial {} error= {}'.format(\"train\",np.round(inierror_train,3))) \n",
    "        print('final {} error= {}'.format(\"train\",np.round(finerror_train,3)))\n",
    "\n",
    "        print('initial {} error= {}'.format(\"test\",np.round(inierror_test,3)))\n",
    "        print('final {} error= {}'.format(\"test\",np.round(finerror_test,3)))\n",
    "\n",
    "        # append the train/test error\n",
    "        #inierror_train,finerror_train,inierror_test,finerror_test = get_error(vx_train,y_train,vx_test,y_test)\n",
    "\n",
    "        train_loss.append(np.array([inierror_train,finerror_train]))\n",
    "        test_loss.append((np.array([inierror_test,finerror_test])))\n",
    "\n",
    "        end = time.perf_counter()\n",
    "        print(f\"Optimization time: {end-start}\")\n",
    "\n",
    "\n",
    "    alpha1min_list_fash_tanh.append(alpha1min_list)\n",
    "    loss_list_fash_tanh.append(loss_list)\n",
    "    train_list_fash_tanh.append(train_loss)\n",
    "    test_list_fash_tanh.append(test_loss)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "# activ = sigmoid  \n",
    "\n",
    "activ=\"sigmoid\"\n",
    "\n",
    "alpha1min_list_fash_sigm = []\n",
    "loss_list_fash_sigm = []\n",
    "train_list_fash_sigm = []\n",
    "test_list_fash_sigm = []\n",
    "\n",
    "\n",
    "    \n",
    "for l in Layers:\n",
    "    \n",
    "    L=l\n",
    "    alpha1min_list = []\n",
    "    loss_list = []\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    opt_time = []\n",
    "\n",
    "\n",
    "    for rate in np.arange(0.1,1.1,0.1):\n",
    "\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        # get the data\n",
    "        loadmnist=False \n",
    "        (vx_train, y_train), (vx_test, y_test) = get_sets(rate)\n",
    "\n",
    "        Ntrain=vx_train.shape[0]\n",
    "        Ntest=vx_test.shape[0]\n",
    "\n",
    "        # optimize over alpha\n",
    "        lr=0.001\n",
    "        alpha1min, loss_history = optimize_alpha1(torch.zeros((D1,p,L),dtype=torch.float64),nsteps=200,lr=lr)\n",
    "        lmin = loss_torch(alpha1min).detach().numpy()\n",
    "\n",
    "        # append the loss / alpha\n",
    "        loss_in=loss_torch(torch.zeros((D1,p,L),dtype=torch.float64)).numpy()\n",
    "        loss_list.append([loss_in,lmin])\n",
    "        alpha1min_list.append(alpha1min.detach().numpy())\n",
    "\n",
    "\n",
    "        # append the train/test error\n",
    "        _,alpha2min=getalpha2_torch(phi_torch(vx_train,alpha1min),y_train)\n",
    "        _,alpha2_init=getalpha2_torch(vx_train,y_train)\n",
    "\n",
    "        inierror_train=get_error(vx_train,y_train,alpha2_init)\n",
    "        finerror_train=get_error(phi_torch(vx_train,alpha1min),y_train,alpha2min)\n",
    "\n",
    "        inierror_test=get_error(vx_test,y_test,alpha2_init)\n",
    "        finerror_test=get_error(phi_torch(vx_test,alpha1min),y_test,alpha2min)\n",
    "\n",
    "        print('initial {} error= {}'.format(\"train\",np.round(inierror_train,3))) \n",
    "        print('final {} error= {}'.format(\"train\",np.round(finerror_train,3)))\n",
    "\n",
    "        print('initial {} error= {}'.format(\"test\",np.round(inierror_test,3)))\n",
    "        print('final {} error= {}'.format(\"test\",np.round(finerror_test,3)))\n",
    "\n",
    "        # append the train/test error\n",
    "        #inierror_train,finerror_train,inierror_test,finerror_test = get_error(vx_train,y_train,vx_test,y_test)\n",
    "\n",
    "        train_loss.append(np.array([inierror_train,finerror_train]))\n",
    "        test_loss.append((np.array([inierror_test,finerror_test])))\n",
    "\n",
    "        end = time.perf_counter()\n",
    "        print(f\"Optimization time: {end-start}\")\n",
    "\n",
    "\n",
    "    alpha1min_list_fash_sigm.append(alpha1min_list)\n",
    "    loss_list_fash_sigm.append(loss_list)\n",
    "    train_list_fash_sigm.append(train_loss)\n",
    "    test_list_fash_sigm.append(test_loss)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store -r alpha1min_mnist1L_sigm_Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_error_NEW(loss_list,train_list,test_list):\n",
    "    #N_list = np.array([50,100,200,400,800,1600])\n",
    "    Layers=np.array([1,2,3])\n",
    "\n",
    "    N_list=[np.int(rate*5000) for rate in np.arange(0.1,1.1,0.1)]\n",
    "    fig = plt.figure(figsize=(15,4)) \n",
    "    c=1\n",
    "    plt.rc('axes', prop_cycle=(cycler('color', ['tab:blue','tab:orange', 'tab:green', 'tab:red', 'tab:purple'])))\n",
    "    \n",
    "    plt.subplot(1, 3, c)\n",
    "    plt.title('Training Error')\n",
    "    plt.plot(N_list,np.array(train_list[0])[:,0],label='Ridge')\n",
    "    for train,L in zip(train_list,Layers):\n",
    "        plt.plot(N_list,np.array(train)[:,1],label=r'$L = {}$'.format(L))\n",
    "    plt.xlabel(r'$N$')\n",
    "    #plt.yscale('simlog')\n",
    "    c+=1\n",
    "\n",
    "    plt.subplot(1, 3, c)\n",
    "    plt.title('Testing Error') \n",
    "    plt.plot(N_list,np.array(test_list[0])[:,0],label='Ridge')\n",
    "    for test,L in zip(test_list,Layers):\n",
    "        plt.plot(N_list,np.array(test)[:,1],label=r'$L = {}$'.format(L))\n",
    "    plt.xlabel(r'$N$')\n",
    "    #plt.yscale('log')\n",
    "    c+=1\n",
    "\n",
    "    plt.subplot(1, 3, c)\n",
    "    plt.title(r\"$L_2$ regularized loss\")\n",
    "    plt.plot(N_list,np.array(loss_list[0])[:,0],label='Ridge')\n",
    "    for l,L in zip(loss_list,Layers):\n",
    "        plt.plot(N_list,np.array(l)[:,1], label=r'$L = {}$'.format(L))\n",
    "    plt.xlabel(r'$N$')\n",
    "    #plt.yscale('log')\n",
    "\n",
    "    plt.legend() #loc=(1.04,0.55)\n",
    "    #plt.tight_layout()\n",
    "\n",
    "    #activ=\"sigmoid\"\n",
    "    #dataset=\"MNIST_fashion\"\n",
    "    #plt.savefig('Error_{}_{}_NEW.pdf'.format(dataset,activ))\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_NEW(loss_list_fash_sigm,train_list_fash_sigm,test_list_fash_sigm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old version of error plots \n",
    "\n",
    "\n",
    "#activ=\"sigmoid\"\n",
    "\n",
    "#Error_plot(np.array(train_mnist1L_relu_small)[:,0],np.array(train_mnist1L_relu_small)[:,1],\n",
    "#         np.array(test_mnist1L_relu_small)[:,0],np.array(test_mnist1L_relu_small)[:,1],\n",
    "#         np.array(loss_mnist1L_relu_small)[:,0],np.array(loss_mnist1L_relu_small)[:,1],\n",
    "#         \"MNIST1L_lr0.001\")\n",
    "\n",
    "#perc_list = np.arange(10,90,10)\n",
    "    #N_list = np.array([50,100,200,400,800,1600])\n",
    "    #N_list = np.arange(10,110,10)\n",
    "N_list=[np.int(rate*5000) for rate in np.arange(0.1,1.1,0.1)]\n",
    "fig = plt.figure(figsize=(15,4)) \n",
    "c=1\n",
    "\n",
    "plt.subplot(1, 3, c)\n",
    "plt.title('Training Error')\n",
    "plt.plot(N_list,np.array(train_mnist1L_sigm_Norm)[:,0],label='Ridge')\n",
    "plt.plot(N_list,np.array(train_mnist1L_sigm_Norm)[:,1], label=r'$L = 1$')\n",
    "plt.plot(N_list,np.array(train_mnist2L_sigm_Norm)[:,1], label=r'$L = 2$')\n",
    "plt.plot(N_list,np.array(train_mnist3L_sigm_Norm)[:,1], label=r'$L = 3$')\n",
    "plt.xlabel(r'$N$')\n",
    "#plt.yscale('simlog')\n",
    "c+=1\n",
    "\n",
    "plt.subplot(1, 3, c)\n",
    "plt.title('Testing Error') \n",
    "plt.plot(N_list,np.array(test_mnist1L_sigm_Norm)[:,0],label='Ridge')\n",
    "plt.plot(N_list,np.array(test_mnist1L_sigm_Norm)[:,1], label=r'$L = 1$')\n",
    "plt.plot(N_list,np.array(test_mnist2L_sigm_Norm)[:,1], label=r'$L = 2$')\n",
    "plt.plot(N_list,np.array(test_mnist3L_sigm_Norm)[:,1], label=r'$L = 3$')\n",
    "plt.xlabel(r'$N$')\n",
    "#plt.ylabel(\"Error\")\n",
    "#plt.yscale('log')\n",
    "c+=1\n",
    "\n",
    "plt.subplot(1, 3, c)\n",
    "plt.title(r\"$L_2$ regularized loss\")\n",
    "plt.plot(N_list,np.array(loss_mnist1L_sigm_Norm)[:,0],label='Ridge')\n",
    "plt.plot(N_list,np.array(loss_mnist1L_sigm_Norm)[:,1], label=r'$L = 1$')\n",
    "plt.plot(N_list,np.array(loss_mnist2L_sigm_Norm)[:,1], label=r'$L = 2$')\n",
    "plt.plot(N_list,np.array(loss_mnist3L_sigm_Norm)[:,1], label=r'$L = 3$')\n",
    "plt.xlabel(r'$N$')\n",
    "#plt.yscale('log')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "#plt.tight_layout()\n",
    "\n",
    "\n",
    "dataset=\"MNIST\"\n",
    "#plt.savefig('Error_{}_{}_NEW.pdf'.format(dataset,activ))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from cycler import cycler\n",
    "\n",
    "def alpha(alpha_relu,alpha_tanh,alpha_sigm):#,alpha_sigm):\n",
    "    Layers=[1,2,3]\n",
    "    c=1\n",
    "    fig = plt.figure(figsize=(15,4)) \n",
    "    Ntrain=5000\n",
    "    N_list=[np.int(rate*Ntrain) for rate in np.arange(0.1,1.1,0.1)]\n",
    "    #N_list = np.array([50,100,200,400,800,1600])\n",
    "    \n",
    "    plt.rc('axes', prop_cycle=(cycler('color', ['tab:orange', 'tab:green', 'tab:red', 'tab:purple'])))\n",
    "    plt.subplot(1, 3, c)\n",
    "    #plt.title('ReLU')\n",
    "    for alpha1min, L in zip(alpha_relu,Layers):\n",
    "        alpha1norm=L*np.sum(np.sum(np.array(alpha1min)**2,axis=(1,2)),axis=1) # (10,) shape\n",
    "        \n",
    "        plt.plot(N_list,alpha1norm,label=r'$L = {}$'.format(L))\n",
    "    plt.xlabel(r'$N$')\n",
    "    #plt.yscale('log')\n",
    "    c+=1\n",
    "    \n",
    "    plt.rc('axes', prop_cycle=(cycler('color', ['tab:orange', 'tab:green', 'tab:red', 'tab:purple'])))\n",
    "    plt.subplot(1, 3, c)\n",
    "    for alpha1min, L in zip(alpha_tanh,Layers):\n",
    "        alpha1norm=L*np.sum(np.sum(np.array(alpha1min)**2,axis=(1,2)),axis=1) # (10,) shape\n",
    "        plt.plot(N_list,alpha1norm,label=r'$L = {}$'.format(L))\n",
    "    plt.xlabel(r'$N$')\n",
    "    #plt.yscale('log')\n",
    "    c+=1\n",
    "    \n",
    "    plt.rc('axes', prop_cycle=(cycler('color', ['tab:orange', 'tab:green', 'tab:red', 'tab:purple'])))\n",
    "    plt.subplot(1, 3, c)\n",
    "    for alpha1min, L in zip(alpha_sigm,Layers):\n",
    "        alpha1norm=L*np.sum(np.sum(np.array(alpha1min)**2,axis=(1,2)),axis=1) # (10,) shape\n",
    "        plt.plot(N_list,alpha1norm,label=r'$L = {}$'.format(L))\n",
    "    plt.xlabel(r'$N$')\n",
    "    #plt.yscale('log')\n",
    "    c+=1\n",
    "    \n",
    "    #plt.subplot(1, 3, c)\n",
    "    #for alpha1min, L in zip(alpha_sigm,Layers):\n",
    "    #    alpha1norm=np.mean(np.sum(np.array(alpha1min)**2,axis=(1,2)),axis=1) # (10,) shape\n",
    "    #    plt.plot(N_list,alpha1norm,label=r'$L = {}$'.format(L))\n",
    "    #plt.xlabel(r'$N_{train}$')\n",
    "    #c+=1\n",
    "    \n",
    "    plt.legend()\n",
    "\n",
    "    #dataset=\"MNIST_fashion\"\n",
    "\n",
    "    #plt.savefig(f'alpha1_NEW_{dataset}.pdf')\n",
    "    plt.show()\n",
    "\n",
    "    #plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha(alpha1min_list_fash_relu,alpha1min_list_fash_tanh,\n",
    "      alpha1min_list_fash_sigm)#,alpha1min_list_synt_sigm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "alpha1_avg=np.mean(np.array(alpha1_mnist3L_sigm_Norm)**2,axis=3) \n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3)\n",
    "fig.set_size_inches(15, 4)\n",
    "for i, ax in zip(np.arange(1,10,4), axes):\n",
    "    ax.imshow(np.log(alpha1_avg[i,:,:]), cmap='hot',interpolation='nearest')\n",
    "    #,norm=LogNorm(vmin=1e-5, vmax=1))#interpolation='nearest')\n",
    "    ax.set_title(f'$N={(i+1)*500}$')\n",
    "    #ax.set_xticks([])\n",
    "    #ax.set_yticks([])\n",
    "\n",
    "#plt.tight_layout()\n",
    "#plt.colorbar(im, cax=axes[4])\n",
    "activ=\"sigmoid\"\n",
    "dataset=\"MNIST_fash\"\n",
    "#plt.savefig('heatplot3L_notLog{}_{}.pdf'.format(dataset,activ))\n",
    "plt.show()\n",
    "\n",
    "#c=1\n",
    "#fig=plt.figure(figsize=(20,4)) \n",
    "\n",
    "#for i in np.arange(1,2,10):\n",
    "#    plt.subplot(1,5,c)\n",
    "#    plt.imshow(\n",
    "#    c+=1\n",
    "#    plt.colorbar()\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store -r alpha1min_list_fash_sigm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Regularization with CIFAR-10  (to complete)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadCIFAR():\n",
    "    Ntrain=5000\n",
    "    Ntest=10000\n",
    "    cifar10 = tf.keras.datasets.cifar10\n",
    "    class_names = ['plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    x_train=x_train.astype('float64')\n",
    "    x_test=x_test.astype('float64')\n",
    "    x_train=x_train[0:Ntrain,:,:]\n",
    "    y_train=y_train[0:Ntrain]\n",
    "    x_test=x_test[0:Ntest,:,:]\n",
    "    y_test=y_test[0:Ntest]\n",
    "    Y_train=np.zeros((Ntrain,10))\n",
    "    Y_test=np.zeros((Ntest,10))\n",
    "    for i in range(10):\n",
    "        Y_train[y_train[0:Ntrain,0]==i,i]=1\n",
    "        Y_test[y_test[0:Ntest,0]==i,i]=1\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    normalizedata=False\n",
    "    if normalizedata:\n",
    "        a=np.sqrt(np.sum(x_train*x_train,(1,2)))\n",
    "        a=a[:,None,None]+np.zeros((Ntrain,32,32))\n",
    "        x_train = np.divide(x_train , a)\n",
    "        a=np.sqrt(np.sum(x_test*x_test,(1,2)))\n",
    "        a=a[:,None,None]+np.zeros((Ntest,32,32))\n",
    "        x_test = np.divide(x_test , a)\n",
    "        a=0 \n",
    "    show=True\n",
    "    if show == True:\n",
    "        def showimages(x_train,y_train):\n",
    "            plt.figure(figsize=(10,10))\n",
    "            for i in range(25):\n",
    "                plt.subplot(5,5,i+1)\n",
    "                plt.xticks([])\n",
    "                plt.yticks([])\n",
    "                plt.grid(False)\n",
    "                plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "                plt.xlabel(class_names[y_train[i,0]])\n",
    "            plt.show()\n",
    "        showimages(x_train,y_train)\n",
    "    d2=32*32*3\n",
    "    vx_train=np.reshape(x_train,(Ntrain,d2))\n",
    "    vx_test=np.reshape(x_test,(Ntest,d2))\n",
    "    return (vx_train,Y_train,vx_test,Y_test)\n",
    "\n",
    "(X_train,Y_train,X_test,Y_test) = LoadCIFAR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntrain=5000\n",
    "def get_sets_CIFAR(rate):\n",
    "    vx_train=X_train[0:np.int(rate*Ntrain),:]\n",
    "    y_train=Y_train[0:np.int(rate*Ntrain),:]\n",
    "\n",
    "    vx_train=torch.tensor(vx_train.astype('float64'))\n",
    "    vx_test=torch.tensor(X_test.astype('float64'))\n",
    "    y_train=torch.tensor(y_train.astype('float64'))\n",
    "    y_test=torch.tensor(Y_test.astype('float64'))\n",
    "\n",
    "    return (vx_train, y_train), (vx_test, y_test)\n",
    "\n",
    "\n",
    "rate=0.5\n",
    "(vx_train, y_train), (vx_test, y_test) = get_sets_CIFAR(rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_reg(x,alpha1,z): #added slack variable z\n",
    "    for i in range(L):\n",
    "        xtopsi1_transp=torch.transpose(xtopsi1_torch(x),0,1)\n",
    "        x=x+torch.matmul(xtopsi1_transp,alpha1[:,:,i])+z[:,:,i]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_reg(alpha1,z):\n",
    "    loss1=torch.sum(alpha1*alpha1) + (1/r)*torch.sum(z*z) #update the loss \n",
    "    loss1=0.5*L*loss1\n",
    "    phivx_train=phi_reg(vx_train,alpha1,z)\n",
    "#    phivx_train=vx_train\n",
    "    (loss2,alpha2)=getalpha2_torch(phivx_train,y_train)\n",
    "    return nu*loss1+loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optim_alpha1_reg(rate,nsteps=1000,lr=0.1):\n",
    "    alpha1_init=torch.zeros((D1,p,L),dtype=torch.float64)\n",
    "    z_init=torch.zeros((np.int(rate*Ntrain),p,L),dtype=torch.float64)\n",
    "    alpha1 = Variable(alpha1_init,requires_grad=True)\n",
    "    z=Variable(z_init,requires_grad=True)\n",
    "    opt = torch.optim.Adam([alpha1,z],lr=lr)\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    for i in range(nsteps):\n",
    "        opt.zero_grad()\n",
    "        stdout.write(\"\\r[%s]\" % (i+1)) \n",
    "        loss = loss_reg(alpha1,z) # Calculate loss function\n",
    "        loss_history.append(loss.detach().numpy()) # Do some recordings for plots\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "       # if i % 50 == 0:\n",
    "       #     print(f'iter {i+1}: loss = {loss}')\n",
    "    \n",
    "    #print(f'final loss: loss = {loss}')\n",
    "\n",
    "    print(r\"Min loss after training alpha and z: {}\".format(np.min(loss_history)))\n",
    "\n",
    "    plt.figure()    \n",
    "    plt.plot(loss_history)                   #plot the loss as a function of iteration i\n",
    "    plt.title('N= {}, lr= {}'.format(len(vx_train),lr))\n",
    "    plt.show()\n",
    "    \n",
    "    return alpha1, z\n",
    "\n",
    "        #stdout.write(\"\\r[%s]\" % loss) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma2=0.01\n",
    "nu=0.01\n",
    "r=0.01\n",
    "L=1\n",
    "\n",
    "p = vx_train.shape[1]\n",
    "#D1 = p + 1\n",
    "D1=200\n",
    "#D2 = vx_train.shape[1]\n",
    "D2=800\n",
    "\n",
    "\n",
    "random1=\"true\"\n",
    "random2=\"true\"\n",
    "activ=\"relu\"\n",
    "\n",
    "Ntrain=5000\n",
    "\n",
    "\n",
    "#alpha1_init=torch.zeros((D1,p,L),dtype=torch.float64)\n",
    "#z_init=torch.zeros((np.int(rate*Ntrain),p,L),dtype=torch.float64)\n",
    "\n",
    "rate=1\n",
    "(vx_train, y_train), (vx_test, y_test) = get_sets_CIFAR(rate)\n",
    "\n",
    "lr=0.0001\n",
    "alpha1min, zmin = optim_alpha1_reg(rate=rate,nsteps=100,lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,alpha2min=getalpha2_torch(phi_torch(vx_train,alpha1min),y_train)\n",
    "_,alpha2_init=getalpha2_torch(vx_train,y_train)\n",
    "        \n",
    "inierror_train=get_error(vx_train,y_train,alpha2_init)#.numpy()\n",
    "finerror_train=get_error(phi_torch(vx_train,alpha1min),y_train,alpha2min)#.detach().numpy()\n",
    "        \n",
    "inierror_test=get_error(vx_test,y_test,alpha2_init)#.numpy()\n",
    "finerror_test=get_error(phi_torch(vx_test,alpha1min),y_test,alpha2min)#.detach().numpy()\n",
    "        \n",
    "print('initial {} error= {}'.format(\"train\",np.round(inierror_train,3))) \n",
    "print('final {} error= {}'.format(\"train\",np.round(finerror_train,3)))\n",
    "    \n",
    "print('initial {} error= {}'.format(\"test\",np.round(inierror_test,3)))\n",
    "print('final {} error= {}'.format(\"test\",np.round(finerror_test,3)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.reshape(phi_reg(vx_train[0:25,:],alpha1min,zmin[0:25,:,:]).detach().numpy(),(25,32,32,3))\n",
    "x2=np.reshape(phi_torch(vx_train[0:25,:],torch.zeros((D1,p,L),dtype=torch.float64)).detach().numpy(),(25,32,32,3))\n",
    "def showimagesCIFAR(x_train):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(25):\n",
    "        plt.subplot(5,5,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "        #plt.title(gamma2)\n",
    "    plt.show()\n",
    "showimagesCIFAR(x2)\n",
    "#showimagesCIFAR(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
